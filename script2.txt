import requests
from bs4 import BeautifulSoup
import random
import time
import logging
from urllib.parse import urljoin

# إعداد تسجيل العمليات
logging.basicConfig(
    filename="darknet_scraper.log",
    level=logging.INFO,
    format="%(asctime)s - %(message)s"
)

# إعدادات بروكسي Tor
proxies = {
    'http': 'socks5h://127.0.0.1:9050',
    'https': 'socks5h://127.0.0.1:9050'
}

# قائمة User-Agent للتدوير
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15)",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:8) Gecko/20100101 Firefox/88.0"
]

# دالة لجلب البيانات من صفحة ويب
def scrape_darknet(url, depth=1):
    """
    Scrape a website up to a specified depth.
    :param url: رابط الموقع
    :param depth: عدد المستويات التي يتم جمع البيانات منها
    """
    try:
        headers = {'User-Agent': random.choice(user_agents)}
        response = requests.get(url, proxies=proxies, headers=headers, timeout=10)
        
        if response.status_code == 200:
            logging.info(f"Successfully fetched: {url}")
            soup = BeautifulSoup(response.text, 'html.parser')
            print(f"Title of {url}: {soup.title.string if soup.title else 'No Title Found'}")
            
            # إذا كان العمق أكبر من 1، تابع الروابط الداخلية
            if depth > 1:
                links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]
                for link in links[:5]:  # فحص أول 5 روابط فقط لتجنب الضغط
                    scrape_darknet(link, depth - 1)
        else:
            logging.warning(f"Failed to fetch {url}: Status code {response.status_code}")
    
    except Exception as e:
        logging.error(f"Error while scraping {url}: {str(e)}")

# تشغيل السكربت
if __name__ == "__main__":
    start_url = "http://exampleonion.com"  # استبدل بعنوان الموقع المطلوب
    scrape_darknet(start_url, depth=2)

