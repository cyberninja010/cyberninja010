import requests
from bs4 import BeautifulSoup
import random
import time
import logging
from urllib.parse import urljoin

# إعداد تسجيل العمليات
logging.basicConfig(
    filename="darknet_scraper.log",
    level=logging.INFO,
    format="%(asctime)s - %(message)s"
)

# إعدادات بروكسي Tor
proxies = {
    'http': 'socks5h://127.0.0.1:9050',
    'https': 'socks5h://127.0.0.1:9050'
}

# قائمة User-Agent للتدوير
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15)",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:8) Gecko/20100101 Firefox/88.0"
]

# قائمة لتجنب زيارات متكررة
visited_links = set()

# دالة لجلب البيانات من صفحة ويب
def scrape_darknet(url, depth=1, timeout=15):
    """
    Scrape a website up to a specified depth.
    :param url: رابط الموقع
    :param depth: عدد المستويات التي يتم جمع البيانات منها
    """
    if url in visited_links:
        return  # تخطي الروابط التي تمت زيارتها مسبقًا
    visited_links.add(url)
    
    try:
        # Set headers and send request
        headers = {"User-Agent": random.choice(user_agents)}
        response = requests.get(url, proxies=proxies, headers=headers, timeout=timeout)

        if response.status_code == 200:
            logging.info(f"Scraped: {url}")
            
            soup = BeautifulSoup(response.text, "html.parser")

            # Extract and display titles
            titles = soup.find_all('h1')
            print("\nTitles Found:")
            for title in titles:
                print(title.get_text(strip=True))

            # Extract and display links
            links = soup.find_all('a')
            print("\nLinks Found:")
            for link in links:
                href = link.get('href')
                if href and href.startswith('http'):
                    print(href)

            # Recursively scrape links
            if depth > 1:
                for link in links:
                    href = link.get('href')
                    if href and href.startswith('http'):
                        # Delay for mimicry
                        time.sleep(random.uniform(1, 3))
                        scrape_darknet(href, depth - 1)
        else:
            logging.warning(f"Failed to scrape {url}: Status {response.status_code}")
    
    except Exception as e:
        logging.error(f"Error scraping {url}: {str(e)}")
        print(f"Error: {e}")

# تشغيل السكربت
if __name__ == "__main__":
    target_url = "http://exampleonionurl.onion"  # استبدل بعنوان الموقع المطلوب
    scrape_darknet(target_url, depth=2)
